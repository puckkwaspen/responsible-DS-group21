{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, matplotlib, sklearn\n",
    "\n",
    "# visualizatoin \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# the data intro has been tested with the following versions\n",
    "print(\"pandas        Tested version: 2.0.3   Your version: %s\" % pd.__version__)\n",
    "print(\"numpy         Tested version: 1.21.5  Your version: %s\" % np.__version__)\n",
    "print(\"matplotlib    Tested version: 3.5.3   Your version: %s\" % matplotlib.__version__)\n",
    "print(\"scikit-learn  Tested version: 1.2.2   Your version: %s\" % sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "If you put the data set in the same folder as this notebook, you can use the following code to load the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description\n",
    "description = pd.read_csv('WiDS_Datathon_2020_Dictionary.csv')\n",
    "description_dict = description.set_index('Variable Name').to_dict(orient='index')\n",
    "# data\n",
    "df = pd.read_csv('training_v2.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The dataset contains many different variables, including:\n",
    "\n",
    "#### Target variable\n",
    "* *hospital_death*: 'Whether the patient died during this hospitalization.\n",
    "\n",
    "#### Identifiers\n",
    "* *patient_id*: Unique identifier associated with a patient\n",
    "* *encounter_id*: Unique identifier associated with a patient unit stay\n",
    "* *hospital_id*: Unique identifier associated with a hospital\n",
    "* *icu_id*: A unique identifier for the unit to which the patient was admitted\n",
    "\n",
    "#### Demographics\n",
    "* *age*: The age of the patient on unit admission.\n",
    "* *bmi*: The body mass index of the person on unit admission.\n",
    "* *ethnicity*: The common national or cultural tradition which the person belongs to.\n",
    "* *gender*: The genotypical sex of the patient.\n",
    "* *height*: The height of the person on unit admission\n",
    "\n",
    "#### Health indicators\n",
    "A few examples:\n",
    "* *elective_surgery*: Whether the patient was admitted to the hospital for an elective surgical operation\n",
    "* *h1_diasbp_invasive_max*: The patient's highest diastolic blood pressure during the first hour of their unit stay, invasively measured\n",
    "* *h1_diasbp_invasive_min*: The patient's lowest diastolic blood pressure during the first hour of their unit stay, invasively measured\n",
    "* *gcs_verbal_apache*: The verbal component of the Glasgow Coma Scale measured during the first 24 hours which results in the highest APACHE III score\n",
    "* *immunosuppression*: Whether the patient has their immune system suppressed within six months prior to ICU admission for any of the following reasons; radiation therapy, chemotherapy, use of non-cytotoxic immunosuppressive drugs, high dose steroids (at least 0.3 mg/kg/day of methylprednisolone or equivalent for at least 6 months)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can retrieve the description of a variable in the data set from the description dictionary as follows\n",
    "description_dict['immunosuppression']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "This is a real-world data set, meaning that it is **messy**. Two key difficulties are categorial features and missing values.\n",
    "\n",
    "> **To avoid information leakage, any pre-processing steps must be based on the training data only.** For example, when we compute the mean, this must be computed based on the training data set.\n",
    "\n",
    "### Categorical Features\n",
    "Many (implementations of) machine learning algorithms cannot handle categorical features automatically. This is often dealt with through *one-hot-encoding*, where each category of a feature is transformed into a binary feature.\n",
    "\n",
    "When a feature contains many categories, this results in a very sparse data set with many features. As such, it can be worthwile to use domain expertise to merge particular categories in order to reduce the number of one-hot-encoded features.\n",
    "\n",
    "### Missing Values\n",
    "The data set contains a lot of missing values (around 35% of the values is missing). There are several ways to deal with this, some ideas to try:\n",
    "\n",
    "* Replace missing values with the mean (numerical features) or median (categorical features), e.g., using [`SimpleImputer`](https://scikit-learn.org/stable/modules/impute.html#univariate-feature-imputation).\n",
    "* Drop features with many missing values.\n",
    "* Model-based imputation strategies, such as [KNNImputer](https://scikit-learn.org/stable/modules/impute.html#nearest-neighbors-imputation).\n",
    "* Domain-knowledge inspired replacement. For example, for features related to medical measurements, it is expected that the entered data is abnormal in some way. As such, replacing by the mean or median can paint a skewed picture. One way to deal with this would be to identify a normal range for different measurements, based on domain expertise. *Note: this is a very time consuming strategy which we do not necessarily recommend in the time span of this project.*\n",
    "\n",
    "We encourage you to try several approaches and see what works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of  Minimal Pre-Processing\n",
    "Below you can find an example of pre-processing the data set for classification. We showcase both 'manual' pre-processing steps through `pandas` as well as a (small) scikit-learn `Pipeline`. Feel free to use whatever you are most comfortable with in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2 # proportion for train versus test+val split\n",
    "val_size = 0.5 # proportion for test versus val split\n",
    "random_state = 42  # random state is used to set a seed for randomness, which is only relevant for reproducibility purposes\n",
    "max_missing = 0.8  # maximum percentage of missing values for a column to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# save features\n",
    "X = df.copy().drop(['hospital_death', 'patient_id', 'encounter_id', 'hospital_id', 'icu_id', # drop identifiers\n",
    "                    'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob', # drop APACHE scores\n",
    "                    'apache_2_bodysystem'], # drop because of similarity with apache_3j_bodysystem\n",
    "                   axis=1)\n",
    "# save target variable\n",
    "y = df['hospital_death'].copy()\n",
    "# save APACHE scores for later evaluation on train / test / validation data\n",
    "y_apache = df['apache_4a_hospital_death_prob'].copy()\n",
    "\n",
    "\"\"\" SPLIT DATA SET \"\"\"\n",
    "# split the dataset into train and test+validation set\n",
    "(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    y_apache_train,\n",
    "    y_apache_test,\n",
    "    ) = train_test_split(X, y, y_apache, \n",
    "                         test_size=test_size, # used for testing and validation\n",
    "                         random_state=random_state # for reproducibility\n",
    "                        ) \n",
    "# split the test set into test + validation set\n",
    "(\n",
    "    X_val,\n",
    "    X_test,\n",
    "    y_val,\n",
    "    y_test,\n",
    "    y_apache_val,\n",
    "    y_apache_test,\n",
    "    ) = train_test_split(X_test, y_test, y_apache_test, \n",
    "                         test_size=val_size, # used for testing and validation\n",
    "                         random_state=random_state # for reproducibility\n",
    "                        ) \n",
    "\n",
    "\"\"\"MISSING VALUES\"\"\"\n",
    "# drop columns with many missing values\n",
    "missing = X_train.isna().sum() > max_missing * len(X_train)\n",
    "missing = missing[missing].index\n",
    "X_train = X_train.drop(missing, axis=1)\n",
    "X_val = X_val.drop(missing, axis=1)\n",
    "X_test = X_test.drop(missing, axis=1)\n",
    "\n",
    "\"\"\"FURTHER PROCESSING PIPELINE\"\"\"\n",
    "# define pre-processing steps for numerical features\n",
    "num_transformer = Pipeline(steps=[(\"constant\", VarianceThreshold()), # remove constant features\n",
    "                                  (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "                                 ])\n",
    "# define preprocessing steps for categorical features\n",
    "cat_transformer = Pipeline(steps=[(\"encoder\", OneHotEncoder(drop='first', sparse_output=False, handle_unknown=\"ignore\"))])\n",
    "# create preprocessing pipeline\n",
    "prep_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, make_column_selector(dtype_exclude=object)), # apply to columns NOT of type object (int or float)\n",
    "        ('cat', cat_transformer, make_column_selector(dtype_include=object)) # apply to columns of type object\n",
    "    ])\n",
    "# pipeline\n",
    "prep_pipeline.fit(X_train, y_train)\n",
    "display(prep_pipeline) # disply preprocessing pipeline\n",
    "\n",
    "# transform data sets\n",
    "X_train = pd.DataFrame(prep_pipeline.transform(X_train), columns=prep_pipeline.get_feature_names_out())\n",
    "X_val = pd.DataFrame(prep_pipeline.transform(X_val), columns=prep_pipeline.get_feature_names_out())\n",
    "X_test = pd.DataFrame(prep_pipeline.transform(X_test), columns=prep_pipeline.get_feature_names_out())\n",
    "        \n",
    "\"\"\"PRINT STATS\"\"\"\n",
    "print(\"Time: %.2fs\" % (time.time() - start_time))\n",
    "print(\"Train set: %s rows, %s columns\" % X_train.shape)\n",
    "print(\"Validation set: %s rows, %s columns\" % X_val.shape)\n",
    "print(\"Test set: %s rows, %s columns\" % X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Logistic Regression Model\n",
    "We can now train a simple logistic regression model.\n",
    "\n",
    "> **Warning**. The following code will lead to a convergence warning. To solve this \"issue\", you can increase `max_iter` and/or apply a `sklearn.preprocessing.StandardScaler()`. However, the model still performance reasonably well even without convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# train logistic regression model\n",
    "lr = LogisticRegression(penalty='l1', solver='saga')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Time: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# APACHE IV Scores\n",
    "APACHE (\"Acute Physiology and Chronic Health Evaluation\") is a scoring system assessing severity of illness and prognoses of ICU patients. The scoring system has been improved over time, with APACHE II being released in 1985, APACHE III in 1991, and finally APACHE IV in 2006. APACHE IV has been evaluated and validated in patients for mortality outcome. \n",
    "\n",
    "In the dataset, the *apache_4a_hospital_death_prob* column corresponds to the APACHE IV probabilistic prediction of in-hospital mortality for the patient which utilizes the APACHE III score and other covariates, including diagnosis.\n",
    "* `-1` means the score couldn't be calculated for some reason. In particular, the patient encounter could have been a re-admission. \n",
    "* `NaN` indicates a missing score, due to e.g., a missing covariate that made it impossible to compute the score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "y_apache_train.hist()\n",
    "plt.title(\"APACHE scores (train)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
